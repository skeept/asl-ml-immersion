{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Continuous training with TFX and Vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Containerize your TFX code into a pipeline package using Cloud Build.\n",
    "1. Use the TFX CLI to compile a TFX pipeline.\n",
    "1. Deploy a TFX pipeline version to run on Vertex Pipelines using the Vertex Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate lab package version installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.6.3\n",
      "TFX version: 1.4.0\n",
      "KFP version: 1.8.10\n",
      "aiplatform: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import tensorflow as tf; print(f'TF version: {tf.__version__}')\"\n",
    "!python -c \"import tfx; print(f'TFX version: {tfx.__version__}')\"\n",
    "!python -c \"import kfp; print(f'KFP version: {kfp.__version__}')\"\n",
    "print(f\"aiplatform: {vertex_ai.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: this lab was built and tested with the following package versions:\n",
    "\n",
    "`TF version: 2.6.2`\n",
    "\n",
    "`TFX version: 1.4.0` \n",
    "\n",
    "`KFP version: 1.8.1`\n",
    "\n",
    "`aiplatform: 1.7.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: example TFX pipeline design pattern for Vertex\n",
    "The pipeline source code can be found in the `pipeline_vertex` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/pipeline/labs/pipeline_vertex\n"
     ]
    }
   ],
   "source": [
    "%cd pipeline_vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48\n",
      "drwxr-xr-x 3 jupyter jupyter 4096 Mar 28 22:35 .\n",
      "drwxr-xr-x 5 jupyter jupyter 4096 Apr  1 16:00 ..\n",
      "-rw-r--r-- 1 jupyter jupyter  131 Mar 28 22:35 Dockerfile\n",
      "-rw-r--r-- 1 jupyter jupyter 1933 Mar 28 22:35 config.py\n",
      "-rw-r--r-- 1 jupyter jupyter 1090 Mar 28 22:35 features.py\n",
      "-rw-r--r-- 1 jupyter jupyter 6482 Mar 28 22:35 model.py\n",
      "-rw-r--r-- 1 jupyter jupyter 5657 Mar 28 22:35 pipeline.py\n",
      "-rw-r--r-- 1 jupyter jupyter 2182 Mar 28 22:35 preprocessing.py\n",
      "-rw-r--r-- 1 jupyter jupyter 1322 Mar 28 22:35 runner.py\n",
      "drwxr-xr-x 2 jupyter jupyter 4096 Mar 28 22:35 schema\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `config.py` module configures the default values for the environment specific settings and the default values for the pipeline runtime parameters. \n",
    "The default values can be overwritten at compile time by providing the updated values in a set of environment variables. You will set custom environment variables later on this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` module contains the TFX DSL defining the workflow implemented by the pipeline.\n",
    "\n",
    "The `preprocessing.py` module implements the data preprocessing logic  the `Transform` component.\n",
    "\n",
    "The `model.py` module implements the TensorFlow model code and training logic for the `Trainer` component.\n",
    "\n",
    "The `runner.py` module configures and executes `KubeflowV2DagRunner`. At compile time, the `KubeflowDagRunner.run()` method converts the TFX DSL into the pipeline package into a JSON format for execution on Vertex.\n",
    "\n",
    "The `features.py` module contains feature definitions common across `preprocessing.py` and `model.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: build your pipeline with the TFX CLI\n",
    "\n",
    "You will use TFX CLI to compile and deploy the pipeline. As explained in the previous section, the environment specific settings can be provided through a set of environment variables and embedded into the pipeline package at compile time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your environment resource settings\n",
    "\n",
    "Update  the below constants  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for AI Platform Training, Vizier, and Prediction.\n",
    "- `ARTIFACT_STORE` - An existing GCS bucket. You can use any bucket, but we will use here the bucket with the same name as the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set your environment resource settings here for GCP_REGION, ARTIFACT_STORE_URI, ENDPOINT, and CUSTOM_SERVICE_ACCOUNT.\n",
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: REGION=us-central1\n",
      "env: ARTIFACT_STORE=gs://qwiklabs-gcp-00-b0ca57451bc1\n",
      "env: PROJECT_ID=qwiklabs-gcp-00-b0ca57451bc1\n"
     ]
    }
   ],
   "source": [
    "# Set your resource settings as environment variables. These override the default values in pipeline/config.py.\n",
    "%env REGION={REGION}\n",
    "%env ARTIFACT_STORE={ARTIFACT_STORE}\n",
    "%env PROJECT_ID={PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-00-b0ca57451bc1/...\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set the compile time settings to first create a pipeline version without hyperparameter tuning\n",
    "\n",
    "Default pipeline runtime environment values are configured in the pipeline folder `config.py`. You will set their values directly below:\n",
    "\n",
    "* `PIPELINE_NAME` - the pipeline's globally unique name.\n",
    "\n",
    "* `DATA_ROOT_URI` - the URI for the raw lab dataset `gs://{PROJECT_ID}/data/tfxcovertype`.\n",
    "\n",
    "* `TFX_IMAGE_URI` - the image name of your pipeline container that will be used to execute each of your tfx components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"tfxcovertype\"\n",
    "DATA_ROOT_URI = f\"gs://{PROJECT_ID}/data/tfxcovertype\"\n",
    "TFX_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{PIPELINE_NAME}\"\n",
    "PIPELINE_JSON = f\"{PIPELINE_NAME}.json\"\n",
    "\n",
    "TRAIN_STEPS = 10\n",
    "EVAL_STEPS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_NAME=tfxcovertype\n",
      "env: DATA_ROOT_URI=gs://qwiklabs-gcp-00-b0ca57451bc1/data/tfxcovertype\n",
      "env: TFX_IMAGE_URI=gcr.io/qwiklabs-gcp-00-b0ca57451bc1/tfxcovertype\n",
      "env: PIPELINE_JSON=tfxcovertype.json\n",
      "env: TRAIN_STEPS=10\n",
      "env: EVAL_STEPS=5\n"
     ]
    }
   ],
   "source": [
    "%env PIPELINE_NAME={PIPELINE_NAME}\n",
    "%env DATA_ROOT_URI={DATA_ROOT_URI}\n",
    "%env TFX_IMAGE_URI={TFX_IMAGE_URI}\n",
    "%env PIPELINE_JSON={PIPELINE_JSON}\n",
    "%env TRAIN_STEPS={TRAIN_STEPS}\n",
    "%env EVAL_STEPS={EVAL_STEPS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us populate the data bucket at `DATA_ROOT_URI`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../../../data/dataset.csv [Content-Type=text/csv]...\n",
      "/ [1 files][  5.3 MiB/  5.3 MiB]                                                \n",
      "Operation completed over 1 objects/5.3 MiB.                                      \n",
      "gs://qwiklabs-gcp-00-b0ca57451bc1/data/tfxcovertype/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp  ../../../data/* $DATA_ROOT_URI/dataset.csv\n",
    "!gsutil ls $DATA_ROOT_URI/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build and push the TFX container image described in the `Dockerfile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 8 file(s) totalling 21.3 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://qwiklabs-gcp-00-b0ca57451bc1_cloudbuild/source/1648829822.444182-f0bc66def81d4e16a565c663b3332445.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-00-b0ca57451bc1/locations/global/builds/654a140e-edd9-4cb9-b0be-d270a5692f3a].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/654a140e-edd9-4cb9-b0be-d270a5692f3a?project=530762185509].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"654a140e-edd9-4cb9-b0be-d270a5692f3a\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-00-b0ca57451bc1_cloudbuild/source/1648829822.444182-f0bc66def81d4e16a565c663b3332445.tgz#1648829822670857\n",
      "Copying gs://qwiklabs-gcp-00-b0ca57451bc1_cloudbuild/source/1648829822.444182-f0bc66def81d4e16a565c663b3332445.tgz#1648829822670857...\n",
      "/ [1 files][  5.8 KiB/  5.8 KiB]                                                \n",
      "Operation completed over 1 objects/5.8 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   29.7kB\n",
      "Step 1/4 : FROM gcr.io/tfx-oss-public/tfx:1.4.0\n",
      "1.4.0: Pulling from tfx-oss-public/tfx\n",
      "e4ca327ec0e7: Pulling fs layer\n",
      "0fa9fc055636: Pulling fs layer\n",
      "448bb2d7fba5: Pulling fs layer\n",
      "a084e2627368: Pulling fs layer\n",
      "de932d3a14d8: Pulling fs layer\n",
      "ebe7db8e97e0: Pulling fs layer\n",
      "66fef8aabad3: Pulling fs layer\n",
      "9696f5331161: Pulling fs layer\n",
      "7799e2177407: Pulling fs layer\n",
      "56d35ebee226: Pulling fs layer\n",
      "f23d5847df7e: Pulling fs layer\n",
      "85729b3aa914: Pulling fs layer\n",
      "27594cd25c9b: Pulling fs layer\n",
      "420dbd17143e: Pulling fs layer\n",
      "71797a45a0de: Pulling fs layer\n",
      "b073ad8b8421: Pulling fs layer\n",
      "4b7516634a4b: Pulling fs layer\n",
      "434af55afd20: Pulling fs layer\n",
      "85d42009e725: Pulling fs layer\n",
      "6d895d7b587a: Pulling fs layer\n",
      "9ff76d709282: Pulling fs layer\n",
      "4160ec915caf: Pulling fs layer\n",
      "eb84f1e37919: Pulling fs layer\n",
      "a616daa88ca5: Pulling fs layer\n",
      "99b8f7435830: Pulling fs layer\n",
      "1e2f495eab09: Pulling fs layer\n",
      "d880b09bf9ec: Pulling fs layer\n",
      "403fe241a00b: Pulling fs layer\n",
      "ca533ed5e802: Pulling fs layer\n",
      "9d118b4c5fa6: Pulling fs layer\n",
      "388915f48178: Pulling fs layer\n",
      "ddeefb06ab95: Pulling fs layer\n",
      "62d0755e4559: Pulling fs layer\n",
      "58a229d727c0: Pulling fs layer\n",
      "c4606a99c9de: Pulling fs layer\n",
      "a084e2627368: Waiting\n",
      "de932d3a14d8: Waiting\n",
      "ebe7db8e97e0: Waiting\n",
      "66fef8aabad3: Waiting\n",
      "9696f5331161: Waiting\n",
      "7799e2177407: Waiting\n",
      "56d35ebee226: Waiting\n",
      "f23d5847df7e: Waiting\n",
      "85729b3aa914: Waiting\n",
      "1e2f495eab09: Waiting\n",
      "27594cd25c9b: Waiting\n",
      "d880b09bf9ec: Waiting\n",
      "420dbd17143e: Waiting\n",
      "71797a45a0de: Waiting\n",
      "403fe241a00b: Waiting\n",
      "b073ad8b8421: Waiting\n",
      "4b7516634a4b: Waiting\n",
      "434af55afd20: Waiting\n",
      "ca533ed5e802: Waiting\n",
      "85d42009e725: Waiting\n",
      "9d118b4c5fa6: Waiting\n",
      "6d895d7b587a: Waiting\n",
      "388915f48178: Waiting\n",
      "ddeefb06ab95: Waiting\n",
      "62d0755e4559: Waiting\n",
      "9ff76d709282: Waiting\n",
      "58a229d727c0: Waiting\n",
      "c4606a99c9de: Waiting\n",
      "4160ec915caf: Waiting\n",
      "eb84f1e37919: Waiting\n",
      "a616daa88ca5: Waiting\n",
      "99b8f7435830: Waiting\n",
      "0fa9fc055636: Verifying Checksum\n",
      "0fa9fc055636: Download complete\n",
      "448bb2d7fba5: Verifying Checksum\n",
      "448bb2d7fba5: Download complete\n",
      "a084e2627368: Verifying Checksum\n",
      "a084e2627368: Download complete\n",
      "de932d3a14d8: Verifying Checksum\n",
      "de932d3a14d8: Download complete\n",
      "66fef8aabad3: Verifying Checksum\n",
      "66fef8aabad3: Download complete\n",
      "e4ca327ec0e7: Verifying Checksum\n",
      "e4ca327ec0e7: Download complete\n",
      "7799e2177407: Verifying Checksum\n",
      "7799e2177407: Download complete\n",
      "e4ca327ec0e7: Pull complete\n",
      "0fa9fc055636: Pull complete\n",
      "448bb2d7fba5: Pull complete\n",
      "9696f5331161: Verifying Checksum\n",
      "9696f5331161: Download complete\n",
      "f23d5847df7e: Verifying Checksum\n",
      "f23d5847df7e: Download complete\n",
      "56d35ebee226: Verifying Checksum\n",
      "56d35ebee226: Download complete\n",
      "a084e2627368: Pull complete\n",
      "de932d3a14d8: Pull complete\n",
      "ebe7db8e97e0: Verifying Checksum\n",
      "ebe7db8e97e0: Download complete\n",
      "420dbd17143e: Verifying Checksum\n",
      "420dbd17143e: Download complete\n",
      "71797a45a0de: Verifying Checksum\n",
      "71797a45a0de: Download complete\n",
      "27594cd25c9b: Verifying Checksum\n",
      "27594cd25c9b: Download complete\n",
      "4b7516634a4b: Verifying Checksum\n",
      "4b7516634a4b: Download complete\n",
      "434af55afd20: Verifying Checksum\n",
      "434af55afd20: Download complete\n",
      "85d42009e725: Verifying Checksum\n",
      "85d42009e725: Download complete\n",
      "6d895d7b587a: Verifying Checksum\n",
      "6d895d7b587a: Download complete\n",
      "9ff76d709282: Verifying Checksum\n",
      "9ff76d709282: Download complete\n",
      "4160ec915caf: Download complete\n",
      "eb84f1e37919: Verifying Checksum\n",
      "eb84f1e37919: Download complete\n",
      "85729b3aa914: Verifying Checksum\n",
      "85729b3aa914: Download complete\n",
      "a616daa88ca5: Verifying Checksum\n",
      "a616daa88ca5: Download complete\n",
      "99b8f7435830: Verifying Checksum\n",
      "99b8f7435830: Download complete\n",
      "b073ad8b8421: Verifying Checksum\n",
      "b073ad8b8421: Download complete\n",
      "403fe241a00b: Download complete\n",
      "1e2f495eab09: Verifying Checksum\n",
      "1e2f495eab09: Download complete\n",
      "9d118b4c5fa6: Verifying Checksum\n",
      "9d118b4c5fa6: Download complete\n",
      "388915f48178: Verifying Checksum\n",
      "388915f48178: Download complete\n",
      "ddeefb06ab95: Verifying Checksum\n",
      "ddeefb06ab95: Download complete\n",
      "62d0755e4559: Verifying Checksum\n",
      "62d0755e4559: Download complete\n",
      "58a229d727c0: Verifying Checksum\n",
      "58a229d727c0: Download complete\n",
      "ca533ed5e802: Verifying Checksum\n",
      "ca533ed5e802: Download complete\n",
      "c4606a99c9de: Verifying Checksum\n",
      "c4606a99c9de: Download complete\n",
      "d880b09bf9ec: Download complete\n",
      "ebe7db8e97e0: Pull complete\n",
      "66fef8aabad3: Pull complete\n",
      "9696f5331161: Pull complete\n",
      "7799e2177407: Pull complete\n",
      "56d35ebee226: Pull complete\n",
      "f23d5847df7e: Pull complete\n",
      "85729b3aa914: Pull complete\n",
      "27594cd25c9b: Pull complete\n",
      "420dbd17143e: Pull complete\n",
      "71797a45a0de: Pull complete\n",
      "b073ad8b8421: Pull complete\n",
      "4b7516634a4b: Pull complete\n",
      "434af55afd20: Pull complete\n",
      "85d42009e725: Pull complete\n",
      "6d895d7b587a: Pull complete\n",
      "9ff76d709282: Pull complete\n",
      "4160ec915caf: Pull complete\n",
      "eb84f1e37919: Pull complete\n",
      "a616daa88ca5: Pull complete\n",
      "99b8f7435830: Pull complete\n",
      "1e2f495eab09: Pull complete\n",
      "d880b09bf9ec: Pull complete\n",
      "403fe241a00b: Pull complete\n",
      "ca533ed5e802: Pull complete\n",
      "9d118b4c5fa6: Pull complete\n",
      "388915f48178: Pull complete\n",
      "ddeefb06ab95: Pull complete\n",
      "62d0755e4559: Pull complete\n",
      "58a229d727c0: Pull complete\n",
      "c4606a99c9de: Pull complete\n",
      "Digest: sha256:1c90d7c7df1d78147013ae8d0377b1496b211391d740b88da6d9892946c30fb1\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.4.0\n",
      " ---> 8badc9bc28b6\n",
      "Step 2/4 : RUN pip install -U pip\n",
      " ---> Running in b86bd34b369e\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (21.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.3.1\n",
      "    Uninstalling pip-21.3.1:\n",
      "      Successfully uninstalled pip-21.3.1\n",
      "Successfully installed pip-22.0.4\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container b86bd34b369e\n",
      " ---> be8d4205eddd\n",
      "Step 3/4 : RUN pip install google-cloud-aiplatform==1.7.1 kfp==1.8.1\n",
      " ---> Running in db6f142f0228\n",
      "Collecting google-cloud-aiplatform==1.7.1\n",
      "  Downloading google_cloud_aiplatform-1.7.1-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 42.9 MB/s eta 0:00:00\n",
      "Collecting kfp==1.8.1\n",
      "  Downloading kfp-1.8.1.tar.gz (248 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 248.5/248.5 KB 26.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.1) (2.30.1)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.1) (1.31.4)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.1) (1.19.7)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.1) (1.42.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.1) (20.9)\n",
      "Collecting absl-py<=0.11,>=0.9\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.8/127.8 KB 18.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (5.4.1)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (12.0.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (1.12.8)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (1.35.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.3/54.3 KB 8.4 MB/s eta 0:00:00\n",
      "Collecting cloudpickle<2,>=1.3.0\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.1.tar.gz (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.2/54.2 KB 5.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 KB 9.3 MB/s eta 0:00:00\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: click<8,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (7.1.2)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.13.tar.gz (23 kB)\n",
      "  Installing build dependencies: started\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TFX_IMAGE_URI ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile your pipeline code\n",
    "\n",
    "The following command will execute the `KubeflowV2DagRunner` that compiles the pipeline described in `pipeline.py` into a JSON representation consumable by Vertex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here to compile the TFX Pipeline using the TFX CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you should see a `{PIPELINE_NAME}.json` file appear in your current pipeline directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: deploy your pipeline on Vertex using the Vertex SDK\n",
    "\n",
    "Once you have the `{PIPELINE_NAME}.json` available, you can run the tfx pipeline on Vertex by launching a pipeline job using the `aiplatform` handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here to use the vertex_ai sdk to deploy your\n",
    "# pipeline image to Vertex Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you learned how to build and deploy a TFX pipeline with the TFX CLI and then update, build and deploy a new pipeline with automatic hyperparameter tuning. You practiced triggered continuous pipeline runs using the TFX CLI as well as the Kubeflow Pipelines UI.\n",
    "\n",
    "\n",
    "In the next lab, you will construct a Cloud Build CI/CD workflow that further automates the building and deployment of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "            http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "tf2-gpu.2-6.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m91"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
